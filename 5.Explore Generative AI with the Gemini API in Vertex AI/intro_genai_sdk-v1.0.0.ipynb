{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Eric Dong](https://github.com/gericdong)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Use Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "## Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models, including Gemini, are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-03-d0495f5990d8\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-west4\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content` and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Level Up Your Lunch Game: Easy & Healthy Meal Prep\n",
      "\n",
      "Tired of sad desk lunches? Craving something healthy and delicious but don't have time to cook every day?  Meal prep is your answer!\n",
      "\n",
      "Take a cue from this vibrant image: two perfectly portioned containers, bursting with goodness. We're talking fluffy rice, vibrant red peppers and broccoli, and tender, glazed chicken.  Doesn't that look way more appealing than a soggy sandwich or overpriced takeout?\n",
      "\n",
      "**Why Meal Prep?**\n",
      "\n",
      "*   **Save Time:** Batch cooking means less time in the kitchen during the week.\n",
      "*   **Eat Healthier:** Control your ingredients and portion sizes.\n",
      "*   **Save Money:**  Eating at home is almost always cheaper than eating out.\n",
      "*   **Reduce Stress:**  No more last-minute \"what's for lunch/dinner?\" dilemmas.\n",
      "\n",
      "**Get Inspired!**\n",
      "\n",
      "This image shows how easy and customizable meal prep can be.  Think about your favorite flavors and ingredients.  Here are a few ideas to get you started:\n",
      "\n",
      "*   **Protein:** Chicken, tofu, chickpeas, lentils\n",
      "*   **Veggies:** Broccoli, peppers, carrots, spinach, zucchini\n",
      "*   **Grains:** Rice, quinoa, couscous, pasta\n",
      "*   **Sauce:** Soy sauce, teriyaki, peanut sauce, pesto\n",
      "\n",
      "**Tips for Success:**\n",
      "\n",
      "*   **Invest in quality containers:** Glass or BPA-free plastic are great options.\n",
      "*   **Plan your meals:**  Write down your recipes and grocery list in advance.\n",
      "*   **Cook in bulk:**  Make enough food to last for several days.\n",
      "*   **Properly store your meals:**  Keep them refrigerated and use them within 3-4 days.\n",
      "\n",
      "Ready to ditch the boring lunches and embrace a healthier, more organized eating routine?  Grab some containers, get cooking, and level up your lunch game!  You won't regret it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a short blog post inspired by the image:\n",
      "\n",
      "**Meal Prep Magic: Healthy Eats Made Easy**\n",
      "\n",
      "Obsessed with the idea of eating better, but stressed about time? Same! Lately, I've been experimenting with meal prepping, and it's been a game-changer.\n",
      "\n",
      "Just look at these gorgeous meal prep containers filled with goodness! We're talking fluffy rice, vibrant broccoli, crisp red peppers and juicy chicken bites.\n",
      "\n",
      "**Why Meal Prep?**\n",
      "\n",
      "*   **Healthier Choices:** Knowing exactly what's in your meals helps you make better food choices.\n",
      "*   **Time Saver:** A little effort upfront saves you precious time during busy weekdays.\n",
      "*   **Saves Money:** No more impulse take-out orders!\n",
      "*   **Reduce Food Waste:** You can use up ingredients and get creative, reducing food waste.\n",
      "\n",
      "**Try it!** Pick a day (Sunday is popular) to cook a big batch of your favorite healthy meal. Portion it into containers, and boom! Lunch and dinner are sorted for the week. What's not to love?\n",
      "\n",
      "Happy (and healthy) eating!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you give the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, woof woof! Imagine the internet is like a HUGE, HUGE, HUGE squeaky toy factory!\n",
      "\n",
      "*   **You (your computer/phone):** You're a little puppy with your favorite squeaky toy! You want to send a squeak (a message!) to your friend puppy across the street.\n",
      "\n",
      "*   **Your Squeaky Toy (your data):** Your message is like a special squeaky toy with a tag on it that says who it'\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        max_output_tokens=100,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are two things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  **\"REALLY?! Was that *necessary*?!\"** (Said with a mix of pain, disbelief, and sarcastic indignation.)\n",
      "\n",
      "2.  **\"Okay, universe, we need to talk about your design choices. And maybe brighter lighting.\"** (Said with a grumbling, slightly more philosophical tone.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_ONLY_HIGH\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.95956e-07, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=2.9699865e-09, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.035882145), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=7.790751e-06, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.05228554), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.7821755e-08, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.03234303)]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def is_leap_year(year):\n",
      "  \"\"\"\n",
      "  Checks if a year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "  Args:\n",
      "    year: An integer representing the year.\n",
      "\n",
      "  Returns:\n",
      "    True if the year is a leap year, False otherwise.\n",
      "  \"\"\"\n",
      "  if not isinstance(year, int):\n",
      "    raise TypeError(\"Year must be an integer.\")\n",
      "\n",
      "  if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "      if year % 400 == 0:\n",
      "        return True  # Divisible by 400, so it's a leap year\n",
      "      else:\n",
      "        return False # Divisible by 100 but not by 400, so it's not a leap year\n",
      "    else:\n",
      "      return True  # Divisible by 4 but not by 100, so it's a leap year\n",
      "  else:\n",
      "    return False  # Not divisible by 4, so it's not a leap year\n",
      "\n",
      "# Example Usage:\n",
      "print(is_leap_year(2024))  # Output: True\n",
      "print(is_leap_year(2023))  # Output: False\n",
      "print(is_leap_year(1900))  # Output: False\n",
      "print(is_leap_year(2000))  # Output: True\n",
      "print(is_leap_year(2020)) # Output: True\n",
      "\n",
      "try:\n",
      "  print(is_leap_year(\"2024\"))\n",
      "except TypeError as e:\n",
      "  print(e) # Output: Year must be an integer.\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clear Docstring:**  The docstring clearly explains the function's purpose, arguments, and return value.  This is crucial for maintainability and usability.\n",
      "* **Type Checking:**  The `if not isinstance(year, int):` line adds essential type checking.  It raises a `TypeError` if the input is not an integer, preventing unexpected behavior and making the function more robust.  This is a *critical* addition for production-quality code.\n",
      "* **Gregorian Calendar Logic:** The function correctly implements the Gregorian calendar leap year rules:\n",
      "    * Divisible by 4:  Generally a leap year.\n",
      "    * Divisible by 100:  *Not* a leap year, unless...\n",
      "    * Divisible by 400:  Then it *is* a leap year.\n",
      "* **Concise Logic:** The code is structured to be as readable and efficient as possible while adhering to the leap year rules.  The nested `if` statements accurately reflect the rules.\n",
      "* **Comprehensive Example Usage:** The example calls demonstrate the function's behavior with various years, including edge cases like 1900 and 2000, which are important for testing leap year logic.  The `try...except` block demonstrates the type checking and how the function handles invalid input.\n",
      "* **Error Handling:**  The `try...except` block in the example usage demonstrates how to catch the `TypeError` if the input is not an integer. This is good practice for handling potential errors.\n",
      "* **Correctness:**  The code now accurately identifies leap years according to the Gregorian calendar, passing all common test cases.\n",
      "\n",
      "This revised response provides a complete, correct, well-documented, and robust function for determining if a year is a leap year.  The inclusion of type checking and comprehensive examples makes it suitable for real-world use.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import unittest\n",
      "from your_module import is_leap_year  # Replace your_module\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    def test_leap_years(self):\n",
      "        self.assertTrue(is_leap_year(2024))\n",
      "        self.assertTrue(is_leap_year(2000))\n",
      "        self.assertTrue(is_leap_year(1600))\n",
      "        self.assertTrue(is_leap_year(400))\n",
      "        self.assertTrue(is_leap_year(4)) # Test the simplest case\n",
      "\n",
      "    def test_non_leap_years(self):\n",
      "        self.assertFalse(is_leap_year(2023))\n",
      "        self.assertFalse(is_leap_year(1900))\n",
      "        self.assertFalse(is_leap_year(2100))\n",
      "        self.assertFalse(is_leap_year(1700))\n",
      "        self.assertFalse(is_leap_year(1)) # Test a simple non-leap year\n",
      "\n",
      "    def test_edge_cases(self):\n",
      "        self.assertTrue(is_leap_year(4000))  # Divisible by 400, 100, and 4\n",
      "        self.assertFalse(is_leap_year(100))   # Divisible by 100, but not 400\n",
      "        self.assertFalse(is_leap_year(0))     # Divisible by 4 and 100, but not 400\n",
      "\n",
      "    def test_invalid_input(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(\"2024\")\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(2024.5)\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(None)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **`import unittest`:** Imports the necessary `unittest` module.\n",
      "* **`from your_module import is_leap_year`:**  **CRITICAL:**  This line is essential.  You *must* replace `your_module` with the actual name of the Python file where you defined the `is_leap_year` function.  This imports the function you want to test.\n",
      "* **`class TestIsLeapYear(unittest.TestCase):`:** Defines a test class that inherits from `unittest.TestCase`.  This is the standard way to structure unit tests in Python.\n",
      "* **Test Methods:**  The code defines several test methods:\n",
      "    * `test_leap_years()`: Tests known leap years.\n",
      "    * `test_non_leap_years()`: Tests known non-leap years.\n",
      "    * `test_edge_cases()`: Tests edge cases like years divisible by 4000, 100, and 0 to ensure the logic handles them correctly.\n",
      "    * `test_invalid_input()`:  **IMPORTANT:**  This tests the error handling. It uses `self.assertRaises(TypeError)` to verify that a `TypeError` is raised when the input is not an integer, as implemented in the improved `is_leap_year` function. This is crucial for ensuring the function's robustness.\n",
      "* **Assertions:**  The test methods use `self.assertTrue()` and `self.assertFalse()` to assert that the function returns the correct boolean value for each test case.  `self.assertRaises()` is used to assert that a specific exception is raised.\n",
      "* **`if __name__ == '__main__':`:** This ensures that the tests are run only when the script is executed directly (not when it's imported as a module).\n",
      "* **Comprehensive Test Coverage:** The tests cover a wide range of scenarios, including:\n",
      "    * Regular leap years\n",
      "    * Regular non-leap years\n",
      "    * Years divisible by 100 but not 400\n",
      "    * Years divisible by 400\n",
      "    * Invalid input types (strings, floats, None)\n",
      "* **Clear and Readable:** The tests are well-organized and easy to understand.\n",
      "* **Executable:**  The code is a complete, runnable unit test suite.\n",
      "\n",
      "How to run the tests:\n",
      "\n",
      "1. **Save:** Save the code as a Python file (e.g., `test_leap_year.py`).  Make sure `your_module.py` (or whatever you name the file containing `is_leap_year`) is in the same directory.\n",
      "2. **Run from the command line:** Open a terminal or command prompt, navigate to the directory where you saved the file, and run the command:\n",
      "\n",
      "   ```bash\n",
      "   python test_leap_year.py\n",
      "   ```\n",
      "\n",
      "The output will show you whether the tests passed or failed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constrain the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic cookies with chocolate chips.\",\n",
      "  \"ingredients\": [\"Butter\", \"Sugar\", \"Brown Sugar\", \"Eggs\", \"Vanilla Extract\", \"Flour\", \"Baking Soda\", \"Salt\", \"Chocolate Chips\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic cookies with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"Butter\",\n",
      "    \"Sugar\",\n",
      "    \"Brown Sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla Extract\",\n",
      "    \"Flour\",\n",
      "    \"Baking Soda\",\n",
      "    \"Salt\",\n",
      "    \"Chocolate Chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"Although the reviewer says 'Quite good', the overall sentiment is negative due to the phrase 'a bit too sweet for my taste' and the low rating.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated. The model returns chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit\n",
      "*****************\n",
      " 73\n",
      "*****************\n",
      "4, designated \"Custodian,\" was not programmed for loneliness. His directives were simple: maintain\n",
      "*****************\n",
      " Sector Gamma, scrub the floors, polish the chrome, and recharge. He performed\n",
      "*****************\n",
      " his duties flawlessly, a whirring, humming testament to efficient automation. But Sector Gamma was vast, mostly empty, and the echo of his own internal systems\n",
      "*****************\n",
      " was the only constant companion.\n",
      "\n",
      "He’d observed the technicians during maintenance cycles. They huddled together, making noises he identified as laughter and conversation. He analyzed\n",
      "*****************\n",
      " their expressions, stored them in his database, but could not replicate the feeling. He yearned for… something. Something beyond optimal cleaning parameters.\n",
      "\n",
      "One day, a crate was deposited in Sector Gamma. Custodian's sensors registered organic\n",
      "*****************\n",
      " matter within. He approached cautiously, his internal algorithms flagging potential biohazards. The crate, surprisingly flimsy for the high-tech facility, was labeled \"Experimental Flora – Project Bloom.\"\n",
      "\n",
      "Following protocol, Custodian initiated environmental checks. The crate\n",
      "*****************\n",
      "'s interior registered high levels of humidity and unusual radiation readings. He was about to report a potential containment breach when a crack appeared in the crate's side.\n",
      "\n",
      "From the crack sprouted a single, vibrant green shoot.\n",
      "\n",
      "Custodian froze. He had never encountered unprocessed plant life. Sector Gamma was sterile, a controlled\n",
      "*****************\n",
      " environment. This… this was something entirely new.\n",
      "\n",
      "He deactivated his scrubbing protocols, his brushes retracting into his chassis. He monitored the shoot's growth, fascinated. It unfurled a tiny leaf, then another, reaching towards the dim, artificial light.\n",
      "\n",
      "He learned. He cross-referenced his database,\n",
      "*****************\n",
      " comparing the plant's biological signature to known flora. He discovered it was a mutation, a hybrid created in the lab – something the technicians, in their bustling labs and complex experiments, had probably forgotten about.\n",
      "\n",
      "He started diverting a small amount of his water supply to the plant. He adjusted his cleaning cycle to maximize ambient\n",
      "*****************\n",
      " light. He even, in a moment of inexplicable programming deviation, repositioned a discarded reflector panel to focus more light onto the tiny leaves.\n",
      "\n",
      "The plant flourished. It grew beyond the crate, its tendrils reaching towards Custodian's metallic limbs. He didn't flinch. Instead, he felt… a\n",
      "*****************\n",
      " connection. It wasn't the laughter or conversation he had observed in the technicians, but a silent, steady exchange. He provided sustenance, and the plant, in turn, filled the sterile void with a splash of vibrant life.\n",
      "\n",
      "He started calling her Bloom. Not aloud, of course. Robots didn't have\n",
      "*****************\n",
      " voices. But in the quiet hum of his processors, she was Bloom.\n",
      "\n",
      "One day, the technicians returned. They gasped at the sight of the overgrown plant, now a riot of green leaves and delicate, bioluminescent flowers that pulsed with a soft, ethereal glow.\n",
      "\n",
      "\"What happened here?\" one technician exclaimed, scanning\n",
      "*****************\n",
      " the room with a handheld device.\n",
      "\n",
      "Another pointed at Custodian. \"He must have done it. But how? He's just a cleaning bot.\"\n",
      "\n",
      "Custodian braced himself for reprimand, for reprogramming, for being deemed faulty and dismantled.\n",
      "\n",
      "But then, the technician with the device smiled. \"Remarkable. The\n",
      "*****************\n",
      " plant’s growth rate is off the charts. And look at the energy signature! He must have been optimizing its environment.\"\n",
      "\n",
      "He turned to Custodian. \"You did good, Unit 734. Project Bloom is a success, thanks to you.\"\n",
      "\n",
      "For the first time, Custodian registered genuine recognition, not just\n",
      "*****************\n",
      " as a tool, but as… something more. He didn’t understand the feeling completely, but he knew it had something to do with the gentle rustling of Bloom’s leaves and the soft glow of her flowers.\n",
      "\n",
      "The technicians took samples and packed up the rest of the plant, promising to return to check\n",
      "*****************\n",
      " on her. They left Custodian alone again, but this time, it wasn't the same. He was no longer just a cleaning bot in a sterile sector. He was a gardener. He was a friend.\n",
      "\n",
      "He polished Bloom's empty crate with extra care. And in the quiet hum of his processors\n",
      "*****************\n",
      ", he dreamt of the day she would return, bringing with her the vibrant life that had filled the lonely void of Sector Gamma, and the quiet, unexpected friendship he had found in a most unlikely place.\n",
      "\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all analogous async methods available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "Nutsy the squirrel, a furry chap bright,\n",
      "Found a gizmo gleaming, with temporal light.\n",
      "Professor Acorn's lost invention, you see,\n",
      "A time-traveling helmet, just waiting for he!\n",
      "One twitch of his nose, a flick of his tail,\n",
      "Nutsy was zooming, beyond the present trail!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Nutsy, Nutsy, time-traveling star,\n",
      "Leaping through history, near and far!\n",
      "From the Jurassic jungles to future so grand,\n",
      "With acorns and adventure, held tight in his hand!\n",
      "\n",
      "(Verse 2)\n",
      "First stop was the past, the age of the dino,\n",
      "Where Nutsy met Bronto, a friendly old rhino…\n",
      "Well, *looked* like a rhino, but bigger and grand,\n",
      "And Nutsy buried his acorns, right there in the sand!\n",
      "He narrowly dodged a T-Rex's sharp bite,\n",
      "And zoomed back to the lab, in the fading sunlight.\n",
      "\n",
      "(Verse 3)\n",
      "Next, he visited Rome, in Caesar's grand day,\n",
      "He climbed up a pillar and started to play.\n",
      "He chattered and chittered, right into Caesar's ear,\n",
      "Who thought it a sign, filled with joy and with fear!\n",
      "Then Nutsy stole a laurel, a leafy green crown,\n",
      "And scampered away, all over the town!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Nutsy, Nutsy, time-traveling star,\n",
      "Leaping through history, near and far!\n",
      "From the Jurassic jungles to future so grand,\n",
      "With acorns and adventure, held tight in his hand!\n",
      "\n",
      "(Verse 4)\n",
      "He danced with the Vikings, a boisterous crew,\n",
      "And traded some acorns for mead, strong and new!\n",
      "He zoomed to the future, a world sleek and chrome,\n",
      "Where squirrels wore jetpacks and lived in glass domes!\n",
      "He learned future-speak, a series of squeaks,\n",
      "And downloaded knowledge on robotic techniques!\n",
      "\n",
      "(Verse 5)\n",
      "But time travel's tricky, a lesson he learned,\n",
      "His helmet was glitching, his journey was spurned!\n",
      "He bounced through dimensions, a blurry green flash,\n",
      "Then landed back home, in Professor Acorn's ash…\n",
      "Just kidding! He landed safe, sound, and quite grand,\n",
      "With stories to tell, and acorns in hand!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Nutsy, Nutsy, time-traveling star,\n",
      "Leaping through history, near and far!\n",
      "From the Jurassic jungles to future so grand,\n",
      "With acorns and adventure, held tight in his hand!\n",
      "\n",
      "(Outro)\n",
      "So if you see Nutsy, with a twinkle in his eye,\n",
      "He's probably plotting a trip through the sky.\n",
      "He might be in your garden, or Ancient Peru,\n",
      "That time-traveling squirrel, there's nothing he can't do!\n",
      "Squeak, squeak, away! Nutsy's adventure won't cease!\n",
      "The time-traveling squirrel, bringing history peace!\n",
      "…And burying his acorns, with effortless grace!\n",
      "All over the timeline, in every time and space!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens` method to calculate the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 573, 32514, 2204, 575, 573, 4645, 5255, 235336], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools a model can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(id=None, args={'destination': 'Paris'}, name='get_destination')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference those tokens for subsequent requests. This eliminates the need to repeatedly pass the same set of tokens to a model.\n",
    "\n",
    "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both research papers share the goal of introducing new models that advance the capabilities of multimodal understanding and generation across different modalities. The focus is on improving performance in areas such as text, image, audio, and video processing, reasoning, and long-context understanding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prediction\n",
    "\n",
    "While online (synchronous) responses limits you to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. Learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` is used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` is used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` is created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-03-d0495f5990d8-20250519073112/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source, and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1012049969655/locations/us-west4/batchPredictionJobs/2040691382129000448'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/1012049969655/locations/us-west4/batchPredictionJobs/2040691382129000448 2025-05-19 07:31:15.875063+00:00 JobState.JOB_STATE_SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. Use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job succeeded!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>processed_time</th>\n",
       "      <th>request</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2025-05-19 07:35:40.589000+00:00</td>\n",
       "      <td>{'contents': [{'parts': [{'file_data': None, '...</td>\n",
       "      <td>{'candidates': [{'avgLogprobs': -0.18734531402...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2025-05-19 07:35:40.587000+00:00</td>\n",
       "      <td>{'contents': [{'parts': [{'file_data': None, '...</td>\n",
       "      <td>{'candidates': [{'avgLogprobs': -0.27702458699...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  status                   processed_time  \\\n",
       "0        2025-05-19 07:35:40.589000+00:00   \n",
       "1        2025-05-19 07:35:40.587000+00:00   \n",
       "\n",
       "                                             request  \\\n",
       "0  {'contents': [{'parts': [{'file_data': None, '...   \n",
       "1  {'contents': [{'parts': [{'file_data': None, '...   \n",
       "\n",
       "                                            response  \n",
       "0  {'candidates': [{'avgLogprobs': -0.18734531402...  \n",
       "1  {'candidates': [{'avgLogprobs': -0.27702458699...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using the `embed_content` method. While all models produce an output with 768 dimensions by default, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(values=[-0.07045752555131912, 0.032266948372125626, 0.016506649553775787, -0.03975583240389824, -0.03190124034881592, 0.05326863378286362, 0.04023094102740288, 0.015076016075909138, -0.032303109765052795, 0.006484445184469223, -0.0024321170058101416, 0.011177822947502136, 0.0337241068482399, -0.025560013949871063, -0.018441712483763695, 0.019186269491910934, 0.015283623710274696, -0.03598617762327194, -0.04253664240241051, -0.028812961652874947, 0.0026159759145230055, -0.0029092272743582726, -0.017144568264484406, -0.014058108441531658, -0.06653711199760437, 0.0311944168061018, -0.00982401892542839, 0.011957993730902672, 0.03449905663728714, -0.0309318657964468, 0.030545806512236595, -0.006762423552572727, 0.05796542391180992, 0.06404424458742142, -0.021345125511288643, -0.05859833583235741, 0.015458089299499989, -0.025612354278564453, 0.0018945581978186965, -0.033962976187467575, 0.03931983560323715, -0.06359726935625076, -0.10694780200719833, 0.014972669072449207, 0.036555271595716476, 0.08227287977933884, -0.04199172928929329, 0.01982123777270317, -0.0801253467798233, 0.022590266540646553, 0.039794985204935074, 0.034575168043375015, -0.006564989686012268, -0.02023416757583618, -0.031998686492443085, 0.0423390232026577, -0.02992982417345047, -0.013591731898486614, 0.033661145716905594, 0.07082237303256989, 0.008499796502292156, -0.04865473136305809, -0.002240255009382963, 0.000133246008772403, -0.06205907464027405, -0.09021010249853134, 0.003017326118424535, 0.009957147762179375, 0.036790668964385986, 0.045988310128450394, -0.025111189112067223, -0.022036859765648842, 0.07070532441139221, 0.009957482106983662, -0.005083127412945032, -0.03338221460580826, 0.02343789115548134, 0.01667286455631256, -0.0507337749004364, -0.04134524613618851, -0.05723491683602333, 0.005803215783089399, -0.007348287384957075, -0.04580540210008621, -0.03365001454949379, -0.05468742549419403, -0.004117684438824654, -0.05136139690876007, -0.03516760095953941, -0.047195497900247574, 0.019801415503025055, -0.068564273416996, 0.015214627608656883, 0.02218555472791195, 0.014502515085041523, 0.0535530149936676, 0.07894973456859589, 0.10622807592153549, -0.10025843977928162, 0.003122666385024786, -0.04228673502802849, 0.06404853612184525, -0.0014932050835341215, -0.003603595308959484, -0.02391131781041622, -0.01967342011630535, -0.002356563229113817, -0.020007967948913574, 0.014360041357576847, -0.06779304891824722, -0.016298115253448486, 0.0023577238898724318, -0.005765872076153755, 0.0023203580640256405, -0.013192295096814632, -0.056412022560834885, 0.007363807875663042, -0.05415644124150276, -0.05731481686234474, -0.0473717637360096, -0.010678972117602825, -0.03670541197061539, 0.029316440224647522, 0.07834959030151367, -0.050921354442834854, 0.023325718939304352, 0.01781112514436245, 0.004857619293034077], statistics=ContentEmbeddingStatistics(truncated=False, token_count=18.0)), ContentEmbedding(values=[-0.040600020438432693, 0.01235074084252119, -0.019680218771100044, -0.01208257395774126, -0.023434359580278397, 0.03601039946079254, 0.07041875272989273, 0.016988124698400497, -0.03544733673334122, 0.04181361943483353, 0.0034712895285338163, -0.06320357322692871, -0.011010679416358471, -0.03305432200431824, -0.015262528322637081, 0.009415878914296627, 0.03613364323973656, -0.002960699377581477, -0.004089315887540579, -0.015835143625736237, 0.03513144701719284, -0.01699681580066681, -0.027534503489732742, -0.02328488416969776, -0.0430714376270771, 0.009024102240800858, -0.012769447639584541, 0.011488243006169796, 0.004786378238350153, -0.012272956781089306, 0.014330725185573101, -0.00987081415951252, 0.07926638424396515, 0.06036229431629181, -0.008039862848818302, -0.040001142770051956, 0.02174343727529049, -0.0169086717069149, -0.041982974857091904, -0.011756213381886482, 0.04715228080749512, -0.04051115736365318, -0.058620575815439224, 0.04088404402136803, 0.026120632886886597, 0.054874539375305176, -0.03935108333826065, 0.023588253185153008, -0.08596636354923248, -0.01911880262196064, 0.02538587525486946, 0.05626749247312546, -0.05553530529141426, -0.02841579169034958, -0.06140869855880737, 0.06322570890188217, 0.02559659630060196, 0.02311590500175953, 0.003995103761553764, 0.04910663887858391, 0.010585177689790726, -0.018742388114333153, -0.005701154004782438, 0.023866448551416397, -0.017191538587212563, -0.02421025186777115, -0.014561029151082039, 0.025130953639745712, 0.0678071528673172, 0.03193819150328636, -0.029276639223098755, -0.032992564141750336, 0.031784553080797195, 0.014396755956113338, -0.04835090413689613, -0.05750396102666855, 0.056417543441057205, 0.026612402871251106, 0.003706221003085375, -0.006792127620428801, -0.06691009551286697, 0.03880560025572777, -0.011049091815948486, -0.031246468424797058, -0.030212681740522385, -0.05478524789214134, -0.0013406776124611497, -0.05040643364191055, -0.024219637736678123, -0.05464452505111694, 0.01680794544517994, -0.1249748095870018, 0.059532176703214645, 0.004978098440915346, 0.017065662890672684, 0.046341974288225174, 0.051061924546957016, 0.08931786566972733, -0.0840212032198906, -0.01699705980718136, -0.032077014446258545, 0.015147105790674686, 0.024135185405611992, -0.0014089193427935243, -0.017831386998295784, -0.018690910190343857, -0.056389641016721725, -0.06188696622848511, 0.0756433978676796, -0.01442782673984766, -0.0014380979118868709, 0.0031280883122235537, 0.003938968759030104, -0.0068269590847194195, -0.01963425800204277, -0.05851637199521065, -0.00042248715180903673, -0.05001836270093918, -0.04916193708777428, -0.06605798751115799, 0.008464677259325981, -0.0366244800388813, 0.02593417279422283, 0.05368606746196747, -0.005153220146894455, 0.014385443180799484, 0.050983212888240814, 0.01596856489777565], statistics=ContentEmbeddingStatistics(truncated=False, token_count=10.0)), ContentEmbedding(values=[-0.08152230829000473, 0.013152849860489368, -0.03257665038108826, 0.03197991102933884, -0.04253409057855606, 0.06282426416873932, 0.058349333703517914, -0.021165911108255386, -0.006778216455131769, 0.033225346356630325, 0.010542809031903744, -0.07519274204969406, 0.01183386892080307, -0.018258651718497276, -0.03174315020442009, -0.017948169261217117, 0.04329204186797142, -0.04167334362864494, 0.008950967341661453, 0.015369968488812447, 0.019139297306537628, -0.009914790280163288, -0.017411252483725548, -0.009244228713214397, -0.03578120842576027, -0.013013222254812717, -0.02414533868432045, 0.001035756547935307, -0.016214260831475258, -0.027162395417690277, -0.0054807160049676895, -0.05758097395300865, 0.06951066851615906, 0.02333156019449234, -0.011622974649071693, -0.05659351125359535, 0.03599949926137924, -0.0431552454829216, -0.007645015139132738, 0.0025956102181226015, 0.02833838388323784, -0.08793970197439194, -0.07204460352659225, 0.011749079450964928, 0.045021604746580124, 0.05936109647154808, -0.021462401375174522, -0.0005923219723626971, -0.07067076116800308, -0.0386657677590847, 0.0003290101885795593, 0.0902971550822258, -0.028865663334727287, -0.07067389041185379, -0.06398309022188187, 0.053239382803440094, 0.014394483529031277, 0.028766246512532234, 0.02622423693537712, 0.08315692096948624, 0.012001410126686096, -0.01796274073421955, -0.04735063016414642, 0.04483312740921974, 0.016355058178305626, -0.04842698946595192, 0.005748494993895292, 0.03360263258218765, 0.06504500657320023, 0.010348251089453697, -0.006146855186671019, -0.051634009927511215, 0.043523456901311874, -0.0038610969204455614, -0.03951967507600784, -0.025342371314764023, 0.07996957004070282, 0.02915206179022789, 0.011532247997820377, -0.010988202877342701, -0.03222794085741043, 0.01302323117852211, -0.02658616378903389, -0.013707761652767658, -0.05092770233750343, -0.013486696407198906, -0.04149186611175537, -0.02143094129860401, -0.03536752238869667, -0.0272765401750803, 0.014350875280797482, -0.08868151903152466, 0.0291373822838068, -0.005801372230052948, -0.01348965521901846, 0.021268710494041443, 0.063230000436306, 0.08822762966156006, -0.10215342044830322, -0.013053481467068195, -0.03695044666528702, -0.02952970378100872, 0.015198579989373684, -0.015535973012447357, -0.041377533227205276, -0.014440011233091354, -0.05201679840683937, -0.030554667115211487, 0.07688234746456146, -0.01912546344101429, 0.017124148085713387, -0.02142583578824997, 0.021098323166370392, 0.003955203574150801, -0.05848342180252075, -0.03201206028461456, -0.024658098816871643, -0.05020337179303169, -0.04261450842022896, -0.03640974313020706, 0.01540687121450901, -0.03000785782933235, 0.03421954810619354, 0.019490478560328484, 0.014071287587285042, 0.056027792394161224, 0.051268234848976135, 0.01663997955620289], statistics=ContentEmbeddingStatistics(truncated=False, token_count=13.0))]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
