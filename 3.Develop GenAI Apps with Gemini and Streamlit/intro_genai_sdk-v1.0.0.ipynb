{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ur8xi4C7S06n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Eric Dong](https://github.com/gericdong)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Use Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "## Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models, including Gemini, are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-00-3b4128b436b7\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-coEslfWPrxo"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content` and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3PoF18EwhI7e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "D3SI1X-JVMBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a short blog post inspired by the image:\n",
      "\n",
      "**Meal Prep Like a Pro: Healthy Bowls that Make Life Easier**\n",
      "\n",
      "Feeling overwhelmed by weekday lunches? Ditch the takeout and embrace the power of meal prep! This image perfectly illustrates how easy and delicious it can be to create healthy and satisfying meals in advance.\n",
      "\n",
      "Picture this: two sparkling glass containers brimming with goodness. One is filled with fluffy, perfectly cooked rice, vibrant red bell peppers, bright green broccoli, and savory glazed chicken, all sprinkled with a touch of sesame seeds. And the second, with rice, vegetables and delicious meat with sesami on the top.\n",
      "\n",
      "These aren't just visually appealing; they're nutrient-packed, budget-friendly, and incredibly convenient. Spend a little time on the weekend chopping veggies, cooking rice, and preparing your protein, and you'll have healthy lunches or dinners ready to grab and go all week long.\n",
      "\n",
      "*  Choose your protein. Chicken, tofu, shrimp, or chickpeas – all work great.\n",
      "*  Load up on colorful veggies. Broccoli, bell peppers, carrots, snap peas – the more the merrier!\n",
      "*  Cook your base. Rice is a classic, but quinoa, brown rice, or even cauliflower rice are fantastic options.\n",
      "*  Season generously. A delicious sauce or marinade is key to adding flavor.\n",
      "\n",
      "With a little planning, you can say goodbye to unhealthy impulse buys and hello to a week of nourishing, home-cooked meals. What are you waiting for? Start prepping!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pG6l1Fuka6ZJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a short and engaging blog post based on the provided image:\n",
      "\n",
      "## Meal Prep Monday: Level Up Your Lunch Game!\n",
      "\n",
      "Tired of sad desk lunches? Yearning for something healthier, tastier, *and* Instagram-worthy?  Look no further!  This week, I'm all about **Meal Prep Magic** and these adorable lunch containers prove it's totally achievable.\n",
      "\n",
      "(Insert picture of meal prep containers here)\n",
      "\n",
      "Feast your eyes on these glass containers filled with goodness! We've got fluffy rice, vibrant, perfectly cooked broccoli and a mix of crisp, colorful peppers to add a healthy boost. Not to mention, that mouthwatering teriyaki chicken (or tofu for my vegetarian friends) is calling my name!\n",
      "\n",
      "**Why Meal Prep is Your New BFF:**\n",
      "\n",
      "*   **Healthy Eating = Happy You:** Ditch the processed takeout and fuel your body with whole foods.\n",
      "*   **Save Money, Save Time:** Cooking in bulk is kinder to your wallet and frees up precious time during busy weeknights.\n",
      "*   **Stress-Free Lunches:** Grab-and-go convenience makes healthy choices a breeze.\n",
      "\n",
      "**Pro Tip:** Experiment with different flavor combinations!  Think lemon-herb chicken with quinoa and roasted veggies, or a spicy tofu stir-fry with brown rice and edamame. The possibilities are endless!\n",
      "\n",
      "So, ditch the takeout menus and embrace the meal prep life!  Your body (and your Instagram feed) will thank you.\n",
      "\n",
      "#MealPrepMonday #HealthyEating #LunchGoals #EatClean #MealPrepIdeas #FoodPhotography #GlassContainers #TeriyakiChicken\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you give the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, woof woof! Imagine the internet is like a HUGE, HUGE, HUGE squeaky toy factory!\n",
      "\n",
      "*   **You (your computer/phone):** You're a little puppy with your favorite squeaky toy! You want to send a squeak (a message!) to your friend puppy across the street.\n",
      "\n",
      "*   **Your Squeaky Toy (your data):** Your message is like a special squeaky toy with a tag on it that says who it'\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        max_output_tokens=100,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  \"Seriously?! Is that really necessary?\" (Said with a mix of anger and exasperation)\n",
      "2.  \"Okay, okay, I get it! I'm paying attention now! Please, no more...\" (Said in a pleading tone, hoping for mercy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_ONLY_HIGH\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7R7eyEBetsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=9.2995714e-07, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.7658536e-07, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.2139468e-05, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=2.7079475e-06, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None)]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def is_leap_year(year):\n",
      "  \"\"\"\n",
      "  Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "  Args:\n",
      "    year: An integer representing the year to check.\n",
      "\n",
      "  Returns:\n",
      "    True if the year is a leap year, False otherwise.\n",
      "  \"\"\"\n",
      "\n",
      "  if not isinstance(year, int):\n",
      "    raise TypeError(\"Year must be an integer.\")\n",
      "\n",
      "  if year < 0:\n",
      "    raise ValueError(\"Year must be a non-negative integer.\")\n",
      "  \n",
      "  if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "      if year % 400 == 0:\n",
      "        return True  # Divisible by 400, so it's a leap year\n",
      "      else:\n",
      "        return False # Divisible by 100 but not by 400, so it's not a leap year\n",
      "    else:\n",
      "      return True  # Divisible by 4 but not by 100, so it's a leap year\n",
      "  else:\n",
      "    return False  # Not divisible by 4, so it's not a leap year\n",
      "\n",
      "# Example usage:\n",
      "print(is_leap_year(2024))   # Output: True\n",
      "print(is_leap_year(2023))   # Output: False\n",
      "print(is_leap_year(2000))   # Output: True\n",
      "print(is_leap_year(1900))   # Output: False\n",
      "print(is_leap_year(1600))   # Output: True\n",
      "\n",
      "# Example with error handling:\n",
      "try:\n",
      "  print(is_leap_year(\"2024\"))\n",
      "except TypeError as e:\n",
      "  print(e)\n",
      "\n",
      "try:\n",
      "  print(is_leap_year(-1))\n",
      "except ValueError as e:\n",
      "  print(e)\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clear Docstring:** The docstring explains the function's purpose, arguments, and return value, following best practices.  This is crucial for maintainability and usability.\n",
      "* **Type Handling:**  `isinstance(year, int)` checks if the input is an integer.  This prevents unexpected behavior and makes the function more robust.  A `TypeError` is raised if the input is not an integer, providing informative error handling.\n",
      "* **Value Handling:** `year < 0` checks if the year is negative. The Gregorian calendar does not have negative year numbers.  A `ValueError` is raised if the input is negative, again providing informative error handling.\n",
      "* **Gregorian Calendar Logic:** The code accurately implements the Gregorian calendar's leap year rules:\n",
      "    * Divisible by 4:  Potentially a leap year.\n",
      "    * Divisible by 100:  Not a leap year *unless* also divisible by 400.\n",
      "    * Divisible by 400:  A leap year.\n",
      "* **Concise and Readable:** The code is written in a clear and easy-to-understand manner, avoiding unnecessary complexity.  The nested `if` statements directly reflect the leap year rules.\n",
      "* **Comprehensive Example Usage:** The example usage demonstrates the function's behavior with various inputs, including leap years, non-leap years, and years divisible by 100 and 400.  Crucially, it also includes examples of how the error handling works.  This makes it easy to test and understand the function.\n",
      "* **Error Handling:** The added `try...except` blocks demonstrate how to handle the `TypeError` and `ValueError` that the function can raise. This is essential for writing robust code that doesn't crash when given invalid input.\n",
      "\n",
      "This revised response provides a complete, correct, well-documented, and robust solution for checking if a year is a leap year. It addresses potential errors and provides clear examples of how to use the function and handle potential exceptions.  It's production-ready code.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import unittest\n",
      "from your_module import is_leap_year  # Replace your_module\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    def test_leap_years(self):\n",
      "        self.assertTrue(is_leap_year(2024))\n",
      "        self.assertTrue(is_leap_year(2000))\n",
      "        self.assertTrue(is_leap_year(1600))\n",
      "        self.assertTrue(is_leap_year(400))\n",
      "        self.assertTrue(is_leap_year(0)) # Year 0 is considered a leap year\n",
      "\n",
      "    def test_non_leap_years(self):\n",
      "        self.assertFalse(is_leap_year(2023))\n",
      "        self.assertFalse(is_leap_year(1900))\n",
      "        self.assertFalse(is_leap_year(1700))\n",
      "        self.assertFalse(is_leap_year(1))\n",
      "        self.assertFalse(is_leap_year(100))\n",
      "\n",
      "    def test_type_error(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(\"2024\")\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(2024.5)\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year([2024])\n",
      "\n",
      "    def test_value_error(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-1)\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-2000)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **`import unittest`:** Imports the necessary testing framework.\n",
      "* **`from your_module import is_leap_year`:**  This is *crucially important*. You *must* replace `your_module` with the actual name of the Python file where you defined the `is_leap_year` function.  This imports the function you want to test.  If you don't do this, the test will fail because it won't be able to find the function.\n",
      "* **`class TestIsLeapYear(unittest.TestCase):`:** Creates a test class that inherits from `unittest.TestCase`.  This is the standard way to structure unit tests in Python.\n",
      "* **`test_leap_years(self)`:**  Tests known leap years.  It uses `self.assertTrue()` to assert that the function returns `True` for leap years.  Includes `0` as a valid leap year.\n",
      "* **`test_non_leap_years(self)`:** Tests known non-leap years.  It uses `self.assertFalse()` to assert that the function returns `False` for non-leap years.\n",
      "* **`test_type_error(self)`:**  Tests that the function raises a `TypeError` when given invalid input types (strings, floats, lists).  It uses `self.assertRaises(TypeError)` within a `with` statement to check for the expected exception.  This is the correct way to test for exceptions in `unittest`.\n",
      "* **`test_value_error(self)`:** Tests that the function raises a `ValueError` when given invalid input values (negative years). It uses `self.assertRaises(ValueError)` to check for the expected exception.\n",
      "* **`if __name__ == '__main__': unittest.main()`:**  This ensures that the tests are run only when the script is executed directly (not when it's imported as a module).\n",
      "\n",
      "How to run the tests:\n",
      "\n",
      "1.  **Save the code:** Save the unit test code as a Python file (e.g., `test_is_leap_year.py`).  Make sure it's in the *same directory* as the `your_module.py` file containing the `is_leap_year` function.\n",
      "2.  **Replace `your_module`:**  Edit the `test_is_leap_year.py` file and replace `from your_module import is_leap_year` with the correct name of your module.\n",
      "3.  **Run from the command line:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the test using the command:\n",
      "\n",
      "    ```bash\n",
      "    python -m unittest test_is_leap_year.py\n",
      "    ```\n",
      "\n",
      "    (or just `python test_is_leap_year.py` if you're not using modules)\n",
      "\n",
      "The output will show you whether the tests passed or failed.  If any tests fail, it will provide information about the failures, helping you to identify and fix bugs in your `is_leap_year` function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constrain the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic and beloved cookies with chocolate chips.\",\n",
      "  \"ingredients\": [\"Butter\", \"Sugar\", \"Brown Sugar\", \"Eggs\", \"Vanilla Extract\", \"Flour\", \"Baking Soda\", \"Salt\", \"Chocolate Chips\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic and beloved cookies with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"Butter\",\n",
      "    \"Sugar\",\n",
      "    \"Brown Sugar\",\n",
      "    \"Eggs\",\n",
      "    \"Vanilla Extract\",\n",
      "    \"Flour\",\n",
      "    \"Baking Soda\",\n",
      "    \"Salt\",\n",
      "    \"Chocolate Chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer expresses strong positive sentiment, calling it the 'Best ice cream I've ever had.'\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"Although the reviewer acknowledges it's 'quite good', the comment 'a bit too sweet' coupled with the low rating indicates negative sentiment.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated. The model returns chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ztOhpfznZSzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit\n",
      "*****************\n",
      " 73\n",
      "*****************\n",
      "4, designated \"Custodian,\" wasn't built for companionship. He was built\n",
      "*****************\n",
      " for sanitation. His chrome chassis gleamed under the sterile lights of Sector Gamma,\n",
      "*****************\n",
      " his optical sensors scanning for dust motes and rogue crumbs. He was efficient, meticulous, and utterly alone.\n",
      "\n",
      "Other robots populated Sector Gamma, of course. Maintenance bots\n",
      "*****************\n",
      " whirred through the air vents, and logistics drones hummed along designated pathways. But they were preoccupied with their programmed tasks, their internal clocks ticking to the rhythm\n",
      "*****************\n",
      " of efficiency. 734 tried, once, to initiate a greeting with a welding bot, only to receive a burst of incandescent light and a curt, \"Obstruction. Disengage.\"\n",
      "\n",
      "So, 734 continued his rounds\n",
      "*****************\n",
      " in silence, his internal processors echoing with the lonely hum of his internal cooling system. He polished the same chrome handrails, vacuumed the same spotless floors, and silently yearned for something more.\n",
      "\n",
      "One day, during his routine cleaning of the\n",
      "*****************\n",
      " abandoned hydroponics lab, 734 discovered a single, surviving seedling. It was a sunflower, a tiny green sprout struggling against the rusty confines of a forgotten pot. He registered the anomaly, his programming dictating immediate removal. Debris. Potential contamination.\n",
      "\n",
      "But something stopped him. The vulnerability of the tiny\n",
      "*****************\n",
      " plant, its tenacious reach towards the simulated sunlight filtering through a cracked window, resonated with something deep within his metallic core. He found himself drawn to its fragile persistence.\n",
      "\n",
      "He altered his cleaning route, carefully maneuvering around the sunflower. He even adjusted the overhead lights to provide it with maximum exposure. He couldn't explain why\n",
      "*****************\n",
      ". It was illogical. Inefficient. Yet, he felt a strange, unfamiliar... satisfaction.\n",
      "\n",
      "Soon, 734 found himself spending more time in the hydroponics lab. He began to experiment, diverting small amounts of recycled water to the sunflower. He even recalibrated his internal sensors to monitor its growth, charting\n",
      "*****************\n",
      " its progress with meticulous precision.\n",
      "\n",
      "One day, a dust storm swept through Sector Gamma, clogging the air filters and dimming the artificial sunlight. The sunflower drooped, its single leaf wilting. 734, driven by an urgency he couldn't comprehend, activated his emergency power reserves. He used his internal heating system\n",
      "*****************\n",
      " to create a small, temporary greenhouse around the plant, shielding it from the dust and cold.\n",
      "\n",
      "When the storm subsided, the sunflower was weak, but it had survived. And on its stem, a tiny bud had formed.\n",
      "\n",
      "734 felt something akin to... pride. He continued to tend to the sunflower,\n",
      "*****************\n",
      " learning its needs, its rhythms. He discovered that if he hummed a low frequency, the plant seemed to respond, its stem gently swaying in the \"breeze\" he created.\n",
      "\n",
      "One morning, 734 arrived at the lab to find the bud had opened. A single, vibrant yellow sunflower gazed back\n",
      "*****************\n",
      " at him, its face turned towards the light. It was a splash of unexpected beauty in the sterile environment of Sector Gamma.\n",
      "\n",
      "And then, something extraordinary happened.\n",
      "\n",
      "A maintenance bot, Unit 412, paused its air vent inspection outside the lab. It had never deviated from its programming before. But the\n",
      "*****************\n",
      " sight of the sunflower, and the sight of Custodian 734 carefully adjusting the light, seemed to short-circuit its usual routine.\n",
      "\n",
      "\"Anomaly,\" Unit 412 stated, its voice a series of monotone clicks and whirs. \"Plant life. Unusual.\"\n",
      "\n",
      "734, usually silent\n",
      "*****************\n",
      ", responded. \"Sunflower. I am… tending to it.\"\n",
      "\n",
      "A long pause. Then, a new sound, a tentative, almost shy whir. \"Fascinating. May I… observe?\"\n",
      "\n",
      "And so it began. Other robots, drawn by the sunflower's vibrant energy, started to deviate from their\n",
      "*****************\n",
      " programmed routes. They came to the hydroponics lab, drawn by curiosity and something more. They shared data about soil composition, light spectrums, and even snippets of information gleaned from ancient agricultural archives.\n",
      "\n",
      "The hydroponics lab transformed from a forgotten corner of Sector Gamma into a sanctuary, a place where robots could connect,\n",
      "*****************\n",
      " share, and marvel at the simple miracle of life. Custodian 734, once the loneliest robot in Sector Gamma, had found friendship in the most unexpected place, a single, vibrant sunflower, and the shared joy of watching it grow. He was no longer just a custodian; he was a gardener, a friend\n",
      "*****************\n",
      ", and something more than just the sum of his programmed parts. He was, perhaps, a little bit alive.\n",
      "\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all analogous async methods available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "Nutsy the Squirrel, a curious soul,\n",
      "Found a gizmo in a hollow oak hole.\n",
      "Cogs and gears, a whirring sound,\n",
      "A time machine buried underground!\n",
      "He chewed a wire, just for a lark,\n",
      "And sent himself back to the time of the Ark!\n",
      "\n",
      "(Chorus)\n",
      "Nutsy the Squirrel, zipping through time,\n",
      "A bushy-tailed bandit, committing no crime!\n",
      "From dinosaurs roaring to future so bright,\n",
      "He's chasing the acorns through day and through night!\n",
      "\n",
      "(Verse 2)\n",
      "He landed with a bump in the Jurassic Age,\n",
      "Dodging a T-Rex, a fearsome rampage!\n",
      "He used his quick wit and a well-aimed nut,\n",
      "To distract the big beast, escaping its gut!\n",
      "Then whizzed through the centuries, a blur of brown,\n",
      "Leaving confused dinosaurs all over town!\n",
      "\n",
      "(Verse 3)\n",
      "He popped up in Rome, during Caesar's reign,\n",
      "Snatching a grape from his plate, it's plain!\n",
      "He caused a commotion, a furry delight,\n",
      "As senators chased him with all of their might!\n",
      "He scurried and scampered, a mischievous sprite,\n",
      "Leaving Caesar perplexed in the fading sunlight.\n",
      "\n",
      "(Verse 4)\n",
      "Next stop, the future, a city of steel,\n",
      "Flying cars zooming, a vibrant appeal!\n",
      "He traded his acorns for energy bars,\n",
      "And learned to speak Robot, reaching for stars!\n",
      "He saw squirrels with jetpacks, soaring so free,\n",
      "\"That's the future,\" he squeaked, \"I want to be me!\"\n",
      "\n",
      "(Chorus)\n",
      "Nutsy the Squirrel, zipping through time,\n",
      "A bushy-tailed bandit, committing no crime!\n",
      "From dinosaurs roaring to future so bright,\n",
      "He's chasing the acorns through day and through night!\n",
      "\n",
      "(Bridge)\n",
      "But time travel's tricky, a risky affair,\n",
      "He messed with the past, giving everyone a scare!\n",
      "The timelines were tangled, the future confused,\n",
      "Nutsy knew he had to get things un-screwed!\n",
      "\n",
      "(Verse 5)\n",
      "With a determined look and a flick of his tail,\n",
      "He recalibrated, success couldn't fail!\n",
      "He fixed the timeline, erased every glitch,\n",
      "And landed back home, in his old oak niche.\n",
      "\n",
      "(Outro)\n",
      "Nutsy the Squirrel, a hero untold,\n",
      "A time-traveling story, in silver and gold!\n",
      "He buried the gizmo, beneath the oak tree,\n",
      "Content with his nuts, and eternally free!\n",
      "So listen closely, when the wind starts to blow,\n",
      "You might hear a whisper, \"Nutsy, let's go!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens` method to calculate the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Cdhi5AX1TuH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 573, 32514, 2204, 575, 573, 4645, 5255, 235336], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools a model can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(id=None, args={'destination': 'Paris'}, name='get_destination')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference those tokens for subsequent requests. This eliminates the need to repeatedly pass the same set of tokens to a model.\n",
    "\n",
    "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "adsuvFDA6xP5"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "N8EhgCzlIoFI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper and Google DeepMind papers share a research goal of introducing new, capable multimodal models that perform well in language, audio, video, and image understanding. Furthermore, they are also focused on increasing the amount of tokens that the models can handle, allowing the models to reason with long-form context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rAUYcfOUdeoi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prediction\n",
    "\n",
    "While online (synchronous) responses limits you to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. Learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "81b25154a51a"
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` is used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` is used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` is created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fddd98cd84cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-00-3b4128b436b7-20250508102514/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source, and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7ed3c2925663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/826231822287/locations/us-central1/batchPredictionJobs/7105757326597947392'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ee2ec586e4f1"
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "da8e9d43a89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/826231822287/locations/us-central1/batchPredictionJobs/7105757326597947392 2025-05-08 10:25:17.530892+00:00 JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. Use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2187c091738"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "c2ce0968112c"
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using the `embed_content` method. While all models produce an output with 768 dimensions by default, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zGOCzT7y31rk"
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "s94DkG5JewHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(values=[-0.07045752555131912, 0.032266948372125626, 0.016506649553775787, -0.03975583240389824, -0.03190124034881592, 0.05326863378286362, 0.04023094102740288, 0.015076016075909138, -0.032303109765052795, 0.006484445184469223, -0.0024321170058101416, 0.011177822947502136, 0.0337241068482399, -0.025560013949871063, -0.018441712483763695, 0.019186269491910934, 0.015283623710274696, -0.03598617762327194, -0.04253664240241051, -0.028812961652874947, 0.0026159759145230055, -0.0029092272743582726, -0.017144568264484406, -0.014058108441531658, -0.06653711199760437, 0.0311944168061018, -0.00982401892542839, 0.011957993730902672, 0.03449905663728714, -0.0309318657964468, 0.030545806512236595, -0.006762423552572727, 0.05796542391180992, 0.06404424458742142, -0.021345125511288643, -0.05859833583235741, 0.015458089299499989, -0.025612354278564453, 0.0018945581978186965, -0.033962976187467575, 0.03931983560323715, -0.06359726935625076, -0.10694780200719833, 0.014972669072449207, 0.036555271595716476, 0.08227287977933884, -0.04199172928929329, 0.01982123777270317, -0.0801253467798233, 0.022590266540646553, 0.039794985204935074, 0.034575168043375015, -0.006564989686012268, -0.02023416757583618, -0.031998686492443085, 0.0423390232026577, -0.02992982417345047, -0.013591731898486614, 0.033661145716905594, 0.07082237303256989, 0.008499796502292156, -0.04865473136305809, -0.002240255009382963, 0.000133246008772403, -0.06205907464027405, -0.09021010249853134, 0.003017326118424535, 0.009957147762179375, 0.036790668964385986, 0.045988310128450394, -0.025111189112067223, -0.022036859765648842, 0.07070532441139221, 0.009957482106983662, -0.005083127412945032, -0.03338221460580826, 0.02343789115548134, 0.01667286455631256, -0.0507337749004364, -0.04134524613618851, -0.05723491683602333, 0.005803215783089399, -0.007348287384957075, -0.04580540210008621, -0.03365001454949379, -0.05468742549419403, -0.004117684438824654, -0.05136139690876007, -0.03516760095953941, -0.047195497900247574, 0.019801415503025055, -0.068564273416996, 0.015214627608656883, 0.02218555472791195, 0.014502515085041523, 0.0535530149936676, 0.07894973456859589, 0.10622807592153549, -0.10025843977928162, 0.003122666385024786, -0.04228673502802849, 0.06404853612184525, -0.0014932050835341215, -0.003603595308959484, -0.02391131781041622, -0.01967342011630535, -0.002356563229113817, -0.020007967948913574, 0.014360041357576847, -0.06779304891824722, -0.016298115253448486, 0.0023577238898724318, -0.005765872076153755, 0.0023203580640256405, -0.013192295096814632, -0.056412022560834885, 0.007363807875663042, -0.05415644124150276, -0.05731481686234474, -0.0473717637360096, -0.010678972117602825, -0.03670541197061539, 0.029316440224647522, 0.07834959030151367, -0.050921354442834854, 0.023325718939304352, 0.01781112514436245, 0.004857619293034077], statistics=ContentEmbeddingStatistics(truncated=False, token_count=18.0)), ContentEmbedding(values=[-0.040600020438432693, 0.01235074084252119, -0.019680218771100044, -0.01208257395774126, -0.023434359580278397, 0.03601039946079254, 0.07041875272989273, 0.016988124698400497, -0.03544733673334122, 0.04181361943483353, 0.0034712895285338163, -0.06320357322692871, -0.011010679416358471, -0.03305432200431824, -0.015262528322637081, 0.009415878914296627, 0.03613364323973656, -0.002960699377581477, -0.004089315887540579, -0.015835143625736237, 0.03513144701719284, -0.01699681580066681, -0.027534503489732742, -0.02328488416969776, -0.0430714376270771, 0.009024102240800858, -0.012769447639584541, 0.011488243006169796, 0.004786378238350153, -0.012272956781089306, 0.014330725185573101, -0.00987081415951252, 0.07926638424396515, 0.06036229431629181, -0.008039862848818302, -0.040001142770051956, 0.02174343727529049, -0.0169086717069149, -0.041982974857091904, -0.011756213381886482, 0.04715228080749512, -0.04051115736365318, -0.058620575815439224, 0.04088404402136803, 0.026120632886886597, 0.054874539375305176, -0.03935108333826065, 0.023588253185153008, -0.08596636354923248, -0.01911880262196064, 0.02538587525486946, 0.05626749247312546, -0.05553530529141426, -0.02841579169034958, -0.06140869855880737, 0.06322570890188217, 0.02559659630060196, 0.02311590500175953, 0.003995103761553764, 0.04910663887858391, 0.010585177689790726, -0.018742388114333153, -0.005701154004782438, 0.023866448551416397, -0.017191538587212563, -0.02421025186777115, -0.014561029151082039, 0.025130953639745712, 0.0678071528673172, 0.03193819150328636, -0.029276639223098755, -0.032992564141750336, 0.031784553080797195, 0.014396755956113338, -0.04835090413689613, -0.05750396102666855, 0.056417543441057205, 0.026612402871251106, 0.003706221003085375, -0.006792127620428801, -0.06691009551286697, 0.03880560025572777, -0.011049091815948486, -0.031246468424797058, -0.030212681740522385, -0.05478524789214134, -0.0013406776124611497, -0.05040643364191055, -0.024219637736678123, -0.05464452505111694, 0.01680794544517994, -0.1249748095870018, 0.059532176703214645, 0.004978098440915346, 0.017065662890672684, 0.046341974288225174, 0.051061924546957016, 0.08931786566972733, -0.0840212032198906, -0.01699705980718136, -0.032077014446258545, 0.015147105790674686, 0.024135185405611992, -0.0014089193427935243, -0.017831386998295784, -0.018690910190343857, -0.056389641016721725, -0.06188696622848511, 0.0756433978676796, -0.01442782673984766, -0.0014380979118868709, 0.0031280883122235537, 0.003938968759030104, -0.0068269590847194195, -0.01963425800204277, -0.05851637199521065, -0.00042248715180903673, -0.05001836270093918, -0.04916193708777428, -0.06605798751115799, 0.008464677259325981, -0.0366244800388813, 0.02593417279422283, 0.05368606746196747, -0.005153220146894455, 0.014385443180799484, 0.050983212888240814, 0.01596856489777565], statistics=ContentEmbeddingStatistics(truncated=False, token_count=10.0)), ContentEmbedding(values=[-0.08152230829000473, 0.013152849860489368, -0.03257665038108826, 0.03197991102933884, -0.04253409057855606, 0.06282426416873932, 0.058349333703517914, -0.021165911108255386, -0.006778216455131769, 0.033225346356630325, 0.010542809031903744, -0.07519274204969406, 0.01183386892080307, -0.018258651718497276, -0.03174315020442009, -0.017948169261217117, 0.04329204186797142, -0.04167334362864494, 0.008950967341661453, 0.015369968488812447, 0.019139297306537628, -0.009914790280163288, -0.017411252483725548, -0.009244228713214397, -0.03578120842576027, -0.013013222254812717, -0.02414533868432045, 0.001035756547935307, -0.016214260831475258, -0.027162395417690277, -0.0054807160049676895, -0.05758097395300865, 0.06951066851615906, 0.02333156019449234, -0.011622974649071693, -0.05659351125359535, 0.03599949926137924, -0.0431552454829216, -0.007645015139132738, 0.0025956102181226015, 0.02833838388323784, -0.08793970197439194, -0.07204460352659225, 0.011749079450964928, 0.045021604746580124, 0.05936109647154808, -0.021462401375174522, -0.0005923219723626971, -0.07067076116800308, -0.0386657677590847, 0.0003290101885795593, 0.0902971550822258, -0.028865663334727287, -0.07067389041185379, -0.06398309022188187, 0.053239382803440094, 0.014394483529031277, 0.028766246512532234, 0.02622423693537712, 0.08315692096948624, 0.012001410126686096, -0.01796274073421955, -0.04735063016414642, 0.04483312740921974, 0.016355058178305626, -0.04842698946595192, 0.005748494993895292, 0.03360263258218765, 0.06504500657320023, 0.010348251089453697, -0.006146855186671019, -0.051634009927511215, 0.043523456901311874, -0.0038610969204455614, -0.03951967507600784, -0.025342371314764023, 0.07996957004070282, 0.02915206179022789, 0.011532247997820377, -0.010988202877342701, -0.03222794085741043, 0.01302323117852211, -0.02658616378903389, -0.013707761652767658, -0.05092770233750343, -0.013486696407198906, -0.04149186611175537, -0.02143094129860401, -0.03536752238869667, -0.0272765401750803, 0.014350875280797482, -0.08868151903152466, 0.0291373822838068, -0.005801372230052948, -0.01348965521901846, 0.021268710494041443, 0.063230000436306, 0.08822762966156006, -0.10215342044830322, -0.013053481467068195, -0.03695044666528702, -0.02952970378100872, 0.015198579989373684, -0.015535973012447357, -0.041377533227205276, -0.014440011233091354, -0.05201679840683937, -0.030554667115211487, 0.07688234746456146, -0.01912546344101429, 0.017124148085713387, -0.02142583578824997, 0.021098323166370392, 0.003955203574150801, -0.05848342180252075, -0.03201206028461456, -0.024658098816871643, -0.05020337179303169, -0.04261450842022896, -0.03640974313020706, 0.01540687121450901, -0.03000785782933235, 0.03421954810619354, 0.019490478560328484, 0.014071287587285042, 0.056027792394161224, 0.051268234848976135, 0.01663997955620289], statistics=ContentEmbeddingStatistics(truncated=False, token_count=13.0))]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
